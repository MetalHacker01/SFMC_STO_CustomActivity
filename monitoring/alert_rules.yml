# Prometheus alert rules for Send Time Optimization Activity

groups:
  - name: sto-activity-alerts
    rules:
      # Application availability alerts
      - alert: STOActivityDown
        expr: up{job="sto-activity"} == 0
        for: 1m
        labels:
          severity: critical
          service: sto-activity
        annotations:
          summary: "STO Activity is down"
          description: "STO Activity has been down for more than 1 minute"
          runbook_url: "https://your-docs.com/runbooks/sto-activity-down"

      - alert: STOActivityHealthUnhealthy
        expr: sto_activity_health_status != 1
        for: 2m
        labels:
          severity: critical
          service: sto-activity
        annotations:
          summary: "STO Activity health check failing"
          description: "STO Activity health status is {{ $value }} (1=healthy, 0=unhealthy)"

      # Performance alerts
      - alert: STOHighResponseTime
        expr: sto_activity_http_request_duration_seconds{quantile="0.95"} > 5
        for: 5m
        labels:
          severity: warning
          service: sto-activity
        annotations:
          summary: "High response time detected"
          description: "95th percentile response time is {{ $value }}s (threshold: 5s)"

      - alert: STOHighErrorRate
        expr: rate(sto_activity_http_requests_total{status=~"5.."}[5m]) / rate(sto_activity_http_requests_total[5m]) > 0.05
        for: 3m
        labels:
          severity: warning
          service: sto-activity
        annotations:
          summary: "High error rate detected"
          description: "Error rate is {{ $value | humanizePercentage }} (threshold: 5%)"

      - alert: STOVeryHighErrorRate
        expr: rate(sto_activity_http_requests_total{status=~"5.."}[5m]) / rate(sto_activity_http_requests_total[5m]) > 0.20
        for: 1m
        labels:
          severity: critical
          service: sto-activity
        annotations:
          summary: "Very high error rate detected"
          description: "Error rate is {{ $value | humanizePercentage }} (threshold: 20%)"

      # Resource usage alerts
      - alert: STOHighMemoryUsage
        expr: sto_activity_memory_usage_ratio > 0.85
        for: 5m
        labels:
          severity: warning
          service: sto-activity
        annotations:
          summary: "High memory usage"
          description: "Memory usage is {{ $value | humanizePercentage }} (threshold: 85%)"

      - alert: STOHighCPUUsage
        expr: sto_activity_cpu_usage_ratio > 0.80
        for: 5m
        labels:
          severity: warning
          service: sto-activity
        annotations:
          summary: "High CPU usage"
          description: "CPU usage is {{ $value | humanizePercentage }} (threshold: 80%)"

      # Business logic alerts
      - alert: STOContactProcessingFailures
        expr: rate(sto_activity_contact_processing_failures_total[5m]) > 0.1
        for: 3m
        labels:
          severity: warning
          service: sto-activity
        annotations:
          summary: "High contact processing failure rate"
          description: "Contact processing failure rate is {{ $value }} failures/second"

      - alert: STOTimezoneEngineFailures
        expr: sto_activity_timezone_engine_errors_total > 10
        for: 1m
        labels:
          severity: warning
          service: sto-activity
        annotations:
          summary: "Timezone engine errors detected"
          description: "{{ $value }} timezone engine errors in the last minute"

      - alert: STOHolidayAPIFailures
        expr: rate(sto_activity_holiday_api_failures_total[5m]) > 0.5
        for: 2m
        labels:
          severity: warning
          service: sto-activity
        annotations:
          summary: "Holiday API failure rate high"
          description: "Holiday API failure rate is {{ $value }} failures/second"

      # External dependency alerts
      - alert: STOSFMCAPIFailures
        expr: rate(sto_activity_sfmc_api_failures_total[5m]) > 0.1
        for: 2m
        labels:
          severity: critical
          service: sto-activity
        annotations:
          summary: "SFMC API failures detected"
          description: "SFMC API failure rate is {{ $value }} failures/second"

      - alert: STODataExtensionUpdateFailures
        expr: rate(sto_activity_data_extension_update_failures_total[5m]) > 0.05
        for: 3m
        labels:
          severity: warning
          service: sto-activity
        annotations:
          summary: "Data extension update failures"
          description: "Data extension update failure rate is {{ $value }} failures/second"

  - name: sto-infrastructure-alerts
    rules:
      # System resource alerts
      - alert: STOHighDiskUsage
        expr: (node_filesystem_size_bytes{mountpoint="/"} - node_filesystem_free_bytes{mountpoint="/"}) / node_filesystem_size_bytes{mountpoint="/"} > 0.85
        for: 5m
        labels:
          severity: warning
          service: sto-activity
        annotations:
          summary: "High disk usage"
          description: "Disk usage is {{ $value | humanizePercentage }} (threshold: 85%)"

      - alert: STOSystemMemoryHigh
        expr: (node_memory_MemTotal_bytes - node_memory_MemAvailable_bytes) / node_memory_MemTotal_bytes > 0.90
        for: 5m
        labels:
          severity: warning
          service: sto-activity
        annotations:
          summary: "High system memory usage"
          description: "System memory usage is {{ $value | humanizePercentage }} (threshold: 90%)"

      # Database alerts (if using PostgreSQL)
      - alert: STOPostgreSQLDown
        expr: up{job="postgres-exporter"} == 0
        for: 1m
        labels:
          severity: critical
          service: sto-activity
        annotations:
          summary: "PostgreSQL is down"
          description: "PostgreSQL database is not responding"

      - alert: STOPostgreSQLHighConnections
        expr: pg_stat_database_numbackends / pg_settings_max_connections > 0.8
        for: 5m
        labels:
          severity: warning
          service: sto-activity
        annotations:
          summary: "High PostgreSQL connection usage"
          description: "PostgreSQL connection usage is {{ $value | humanizePercentage }} (threshold: 80%)"

      # Cache alerts (if using Redis)
      - alert: STORedisDown
        expr: up{job="redis-exporter"} == 0
        for: 1m
        labels:
          severity: warning
          service: sto-activity
        annotations:
          summary: "Redis is down"
          description: "Redis cache is not responding"

      - alert: STORedisHighMemoryUsage
        expr: redis_memory_used_bytes / redis_memory_max_bytes > 0.85
        for: 5m
        labels:
          severity: warning
          service: sto-activity
        annotations:
          summary: "High Redis memory usage"
          description: "Redis memory usage is {{ $value | humanizePercentage }} (threshold: 85%)"

  - name: sto-business-alerts
    rules:
      # Journey Builder integration alerts
      - alert: STOJourneyBuilderIntegrationFailures
        expr: rate(sto_activity_journey_builder_failures_total[10m]) > 0.01
        for: 5m
        labels:
          severity: critical
          service: sto-activity
        annotations:
          summary: "Journey Builder integration failures"
          description: "Journey Builder integration failure rate is {{ $value }} failures/second"

      # Send time calculation alerts
      - alert: STOSendTimeCalculationLatency
        expr: sto_activity_send_time_calculation_duration_seconds{quantile="0.95"} > 10
        for: 3m
        labels:
          severity: warning
          service: sto-activity
        annotations:
          summary: "High send time calculation latency"
          description: "95th percentile send time calculation latency is {{ $value }}s (threshold: 10s)"

      # Configuration alerts
      - alert: STOInvalidConfiguration
        expr: sto_activity_configuration_validation_failures_total > 0
        for: 1m
        labels:
          severity: warning
          service: sto-activity
        annotations:
          summary: "Invalid configuration detected"
          description: "{{ $value }} configuration validation failures detected"

      # Data quality alerts
      - alert: STOHighGeosegmentMissingRate
        expr: rate(sto_activity_geosegment_missing_total[10m]) / rate(sto_activity_contacts_processed_total[10m]) > 0.20
        for: 5m
        labels:
          severity: warning
          service: sto-activity
        annotations:
          summary: "High rate of missing geosegment data"
          description: "{{ $value | humanizePercentage }} of contacts are missing geosegment data (threshold: 20%)"

      - alert: STOUnsupportedCountryRate
        expr: rate(sto_activity_unsupported_country_total[10m]) / rate(sto_activity_contacts_processed_total[10m]) > 0.10
        for: 5m
        labels:
          severity: warning
          service: sto-activity
        annotations:
          summary: "High rate of unsupported countries"
          description: "{{ $value | humanizePercentage }} of contacts have unsupported country codes (threshold: 10%)"